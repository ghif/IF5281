{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generasi Teks dengan RNN/LSTM menggunakan JAX dan Flax NNX\n",
                "\n",
                "Notebook ini mendemonstrasikan cara membangun model bahasa sederhana untuk menghasilkan teks secara otomatis (text generation) menggunakan library JAX dan Flax NNX. Kita akan menggunakan kumpulan puisi Chairil Anwar sebagai data latih.\n",
                "\n",
                "## Langkah 1: Persiapan Lingkungan dan Konfigurasi\n",
                "\n",
                "Pertama, kita perlu mengatur path agar notebook dapat menemukan modul pendukung (`model_utils` dan `seq_processor`) serta mengonfigurasi JAX untuk berjalan di CPU guna menghindari masalah kompatibilitas pada beberapa perangkat Mac."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "module 'jax.lib' has no attribute 'xla_bridge'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Paksa penggunaan CPU untuk stabilitas\u001b[39;00m\n\u001b[32m     19\u001b[39m jax.config.update(\u001b[33m\"\u001b[39m\u001b[33mjax_platform_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mJAX Platform:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxla_bridge\u001b[49m.get_backend().platform)\n",
                        "\u001b[31mAttributeError\u001b[39m: module 'jax.lib' has no attribute 'xla_bridge'"
                    ]
                }
            ],
            "source": [
                "import sys, os\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from flax import nnx\n",
                "import optax\n",
                "import numpy as np\n",
                "from time import process_time\n",
                "\n",
                "# Mengatur agar path mengarah ke root directory proyek JAX\n",
                "script_dir = os.getcwd()\n",
                "jax_dir = os.path.dirname(script_dir)\n",
                "if jax_dir not in sys.path:\n",
                "    sys.path.append(jax_dir)\n",
                "\n",
                "import seq_processor as sp\n",
                "import model_utils as mu\n",
                "\n",
                "# # Paksa penggunaan CPU untuk stabilitas\n",
                "# jax.config.update(\"jax_platform_name\", \"cpu\")\n",
                "\n",
                "# print(\"JAX Platform:\", jax.lib.xla_bridge.get_backend().platform)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 2: Memuat dan Memproses Data\n",
                "\n",
                "Kita akan menggunakan file teks berisi puisi-puisi Chairil Anwar. Karakter-karakter dalam teks tersebut akan diubah menjadi representasi numerik (integer) agar dapat diproses oleh model neural network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_dir = \"/Users/mghifary/Work/Code/AI/IF5281/2024/data\"\n",
                "data_path = os.path.join(data_dir, \"chairilanwar.txt\")\n",
                "\n",
                "# Inisialisasi processor karakter\n",
                "chproc = sp.CharProcessor(data_path)\n",
                "data = jnp.array(chproc.encode(chproc.text), dtype=jnp.int32)\n",
                "\n",
                "print(f\"Total karakter: {len(data)}\")\n",
                "print(f\"Ukuran vokabulari: {chproc.vocab_size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 3: Definisi Model dan Optimizer\n",
                "\n",
                "Kita menggunakan arsitektur `SimpleBigram` yang didasarkan pada LSTM. Model ini telah dioptimasi menggunakan `jax.lax.scan` untuk menangani urutan karakter yang panjang secara efisien tanpa memperlambat proses kompilasi JIT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seq_len = 256\n",
                "n_embed = 384\n",
                "n_hidden = 512\n",
                "batch_size = 64\n",
                "\n",
                "# Inisialisasi model\n",
                "rngs = nnx.Rngs(1337)\n",
                "model = mu.SimpleBigram(\n",
                "    chproc.vocab_size,\n",
                "    seq_len,\n",
                "    n_embed,\n",
                "    n_hidden,\n",
                "    num_layers=1,\n",
                "    rngs=rngs\n",
                ")\n",
                "\n",
                "# Inisialisasi optimizer dengan AdamW\n",
                "optimizer = nnx.Optimizer(model, optax.adamw(3e-4), wrt=nnx.Param)\n",
                "\n",
                "print(\"Model siap dilatih.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 4: Fungsi Pelatihan\n",
                "\n",
                "Kita mendefinisikan `loss_fn` untuk menghitung error (cross-entropy) dan `train_step` yang dihiasi dengan `@nnx.jit` untuk mengeksekusi pelatihan secara cepat di XLA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss_fn(model, xb, yb):\n",
                "    logits = model(xb)\n",
                "    B, T, C = logits.shape\n",
                "    logits_flat = logits.reshape(B * T, C)\n",
                "    targets_flat = yb.reshape(B * T)\n",
                "    loss = optax.softmax_cross_entropy_with_integer_labels(logits_flat, targets_flat).mean()\n",
                "    return loss\n",
                "\n",
                "@nnx.jit\n",
                "def train_step(model, optimizer, xb, yb):\n",
                "    loss, grads = nnx.value_and_grad(loss_fn)(model, xb, yb)\n",
                "    optimizer.update(model, grads)\n",
                "    return loss\n",
                "\n",
                "@nnx.jit(static_argnums=(2, 3, 4))\n",
                "def estimate_loss(model, data, eval_iters=10, batch_size=32, seq_len=64, key=None):\n",
                "    model.eval()\n",
                "    losses = []\n",
                "    for k in range(eval_iters):\n",
                "        curr_key = jax.random.fold_in(key, k) if key is not None else jax.random.PRNGKey(k)\n",
                "        xb, yb = sp.get_batch(data, batch_size=batch_size, block_size=seq_len, key=curr_key)\n",
                "        loss = loss_fn(model, xb, yb)\n",
                "        losses.append(loss)\n",
                "    \n",
                "    avg_loss = jnp.mean(jnp.array(losses))\n",
                "    model.train()\n",
                "    return avg_loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 5: Proses Pelatihan\n",
                "\n",
                "Kita akan menjalankan iterasi pelatihan. Setiap interval tertentu, kita akan menghitung rata-rata loss dan mencoba menghasilkan beberapa teks awal untuk melihat perkembangan model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_iters = 100 # Dikurangi untuk demonstrasi cepat\n",
                "eval_interval = 20\n",
                "key = jax.random.PRNGKey(0)\n",
                "\n",
                "print(\"Memulai pelatihan...\")\n",
                "\n",
                "for step in range(max_iters):\n",
                "    key, subkey = jax.random.split(key)\n",
                "    xb, yb = sp.get_batch(data, batch_size=batch_size, block_size=seq_len, key=subkey)\n",
                "\n",
                "    start_t = process_time()\n",
                "    loss = train_step(model, optimizer, xb, yb)\n",
                "    elapsed_t = process_time() - start_t\n",
                "\n",
                "    if step % eval_interval == 0 or step == max_iters - 1:\n",
                "        key, subkey = jax.random.split(key)\n",
                "        train_loss = estimate_loss(model, data, eval_iters=5, batch_size=batch_size, seq_len=seq_len, key=subkey)\n",
                "\n",
                "        print(f\"[Iter-{step+1}/{max_iters}] Loss: {train_loss:.4f} ({elapsed_t:.3f}s)\")\n",
                "        \n",
                "        # Coba hasilkan teks singkat\n",
                "        idx = jnp.zeros((1, 1), dtype=jnp.int32)\n",
                "        pred_idx = model.generate(idx, 50, rngs=rngs)\n",
                "        pred_str = chproc.decode(np.array(pred_idx[0]))\n",
                "        print(f\"--- Teks Tergenerasi ---\\n{pred_str}\\n------------------------\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Langkah 6: Generasi Teks Akhir\n",
                "\n",
                "Setelah model dilatih, kita bisa menghasilkan teks yang lebih panjang."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Generating long text samples...\")\n",
                "idx = jnp.zeros((1, 1), dtype=jnp.int32)\n",
                "pred_idx = model.generate(idx, 200, rngs=rngs)\n",
                "pred_str = chproc.decode(np.array(pred_idx[0]))\n",
                "print(f\"Final Generated text:\\n\\n{pred_str}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "jax-cpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
