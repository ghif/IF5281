{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad62448",
   "metadata": {},
   "source": [
    "# Klasifikasi Gambar dengan Convolutional Neural Network (CNN)\n",
    "\n",
    "Notebook ini mendemonstrasikan cara membangun dan melatih model **CNN** menggunakan ekosistem JAX modern:\n",
    "*   **JAX**: Untuk komputasi performa tinggi.\n",
    "*   **Flax NNX**: API terbaru untuk definisi model.\n",
    "*   **Grain**: Library *data loading* yang cepat dan skalabel.\n",
    "\n",
    "Kita akan melatih model CNN untuk mengenali kategori pakaian dari dataset **Fashion-MNIST**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d3170",
   "metadata": {},
   "source": [
    "### Langkah 1: Persiapan Environment & Impor Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from time import process_time\n",
    "from IPython import display\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import numpy as np\n",
    "import grain.python as grain\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import safetensors\n",
    "from safetensors.flax import save_file, load_file\n",
    "\n",
    "# Module lokal\n",
    "import viz_utils as vu\n",
    "from plot_lib import set_default\n",
    "import train_utils as tu\n",
    "from model_utils import save_checkpoint, load_checkpoint\n",
    "\n",
    "import importlib\n",
    "importlib.reload(vu)\n",
    "importlib.reload(tu)\n",
    "\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b4ba4",
   "metadata": {},
   "source": [
    "### Langkah 2: Konfigurasi Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "MODEL_DIR = \"../models\"\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e770d14",
   "metadata": {},
   "source": [
    "### Langkah 3: Memuat Dataset Fashion-MNIST\n",
    "\n",
    "Kita menggunakan `scikit-learn` untuk mengunduh Fashion-MNIST dan mengemasnya dalam `FashionMNISTSource` yang kompatibel dengan **Grain**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb22b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memuat Fashion-MNIST via OpenML...\")\n",
    "fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False, parser='liac-arff')\n",
    "X_all, y_all = fmnist.data, fmnist.target.astype(np.int32)\n",
    "\n",
    "# Split 60k / 10k\n",
    "X_train, X_test = X_all[:60000], X_all[60000:]\n",
    "y_train, y_test = y_all[:60000], y_all[60000:]\n",
    "\n",
    "class FashionMNISTSource(grain.RandomAccessDataSource):\n",
    "    def __init__(self, images, labels):\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Reshape ke (28, 28, 1) - format JAX standard (H, W, C)\n",
    "        image = self._images[index].reshape(28, 28, 1).astype(np.float32) / 255.0\n",
    "        label = self._labels[index]\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "train_source = FashionMNISTSource(X_train, y_train)\n",
    "test_source = FashionMNISTSource(X_test, y_test)\n",
    "print(f\"Train size: {len(train_source)}, Test size: {len(test_source)}\")\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c08c7c",
   "metadata": {},
   "source": [
    "### Langkah 4: Membangun Data Pipeline dengan Grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(source, batch_size, shuffle=True, seed=42):\n",
    "    sampler = grain.IndexSampler(\n",
    "        num_records=len(source),\n",
    "        num_epochs=1,\n",
    "        shard_options=grain.NoSharding(),\n",
    "        shuffle=shuffle,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    loader = grain.DataLoader(\n",
    "        data_source=source,\n",
    "        sampler=sampler,\n",
    "        worker_count=0, # Sync execution\n",
    "    )\n",
    "\n",
    "    # Convert record dictionaries to tuples for multi-assignment\n",
    "    class BatchIterator:\n",
    "        def __init__(self, loader, batch_size, num_records):\n",
    "            self.loader = loader\n",
    "            self.batch_size = batch_size\n",
    "            self.num_records = num_records\n",
    "        \n",
    "        def __len__(self):\n",
    "            return (self.num_records + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        def __iter__(self):\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for record in self.loader:\n",
    "                batch_images.append(record['image'])\n",
    "                batch_labels.append(record['label'])\n",
    "                if len(batch_images) == self.batch_size:\n",
    "                    yield np.stack(batch_images), np.array(batch_labels)\n",
    "                    batch_images = []\n",
    "                    batch_labels = []\n",
    "            if batch_images:\n",
    "                 yield np.stack(batch_images), np.array(batch_labels)\n",
    "    \n",
    "    return BatchIterator(loader, batch_size, len(source))\n",
    "\n",
    "train_loader = create_loader(train_source, BATCH_SIZE, shuffle=True, seed=SEED)\n",
    "test_loader = create_loader(test_source, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Peek the batched data\n",
    "for ib, (X_batch, y_batch) in enumerate(train_loader):\n",
    "    print(f\"[{ib}] Shape of X [N, C, H, W]: {X_batch.shape}\")\n",
    "    print(f\"[{ib}] Shape of y: {y_batch.shape}, {y_batch.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce652c7",
   "metadata": {},
   "source": [
    "### Langkah 5: Visualisasi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184aedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training samples\n",
    "grid = vu.set_grid(X_batch[:48], num_cells=48)\n",
    "vu.show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f313d31",
   "metadata": {},
   "source": [
    "### Langkah 6: Definisi Model CNN\n",
    "\n",
    "Menggunakan `nnx.Conv` dan `nnx.Linear` untuk membangun arsitektur CNN sederhana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nnx.Module):\n",
    "    def __init__(self, c_in, n_classes, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(c_in, 32, kernel_size=(3, 3), padding='SAME', rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), padding='SAME', rngs=rngs)\n",
    "        self.avg_pool = lambda x: jnp.mean(x, axis=(1, 2)) # Global Average Pooling\n",
    "        self.linear = nnx.Linear(64, n_classes, rngs=rngs)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.conv1(x))\n",
    "        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nnx.relu(self.conv2(x))\n",
    "        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "rngs = nnx.Rngs(SEED)\n",
    "model = ConvNet(1, num_classes, rngs=rngs)\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1313e6",
   "metadata": {},
   "source": [
    "### Langkah 7: Pelatihan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a00ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nnx.Optimizer(model, optax.adam(LEARNING_RATE), wrt=nnx.Param)\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average(),\n",
    ")\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "checkpoint_dir = f\"{MODEL_DIR}/conv2d_v2\"\n",
    "\n",
    "# create checkpoint directory if not exists\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset metrics for each epoch\n",
    "    metrics.reset()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_time = tu.train(model, train_loader, optimizer, metrics)\n",
    "    train_results = metrics.compute()\n",
    "    history['train_loss'].append(train_results['loss'])\n",
    "    history['train_acc'].append(train_results['accuracy'])\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics.reset()\n",
    "    tu.evaluate(model, test_loader, metrics)\n",
    "    test_results = metrics.compute()\n",
    "    history['test_loss'].append(test_results['loss'])\n",
    "    history['test_acc'].append(test_results['accuracy'])\n",
    "    \n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS}] Train Acc: {train_results['accuracy']:.4f}, Test Acc: {test_results['accuracy']:.4f}\")\n",
    "    save_checkpoint(model, epoch + 1, filedir=f\"{checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0d259",
   "metadata": {},
   "source": [
    "### Langkah 8: Visualisasi Hasil Pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b188775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['test_loss'], label='Test')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train')\n",
    "plt.plot(history['test_acc'], label='Test')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f46d71",
   "metadata": {},
   "source": [
    "### Langkah 9: Menyimpan dan Memuat Model\n",
    "\n",
    "Salah satu aspek penting dalam deep learning adalah kemampuan untuk menyimpan state model (parameter) sehingga kita tidak perlu melatih ulang dari awal. Kita menggunakan format `safetensors` dari Huggingfacekarena aman, efisien, dan cepat.\n",
    "\n",
    "**Alasan Menggunakan Safetensors:**\n",
    "1.  **Keamanan**: Tidak mengizinkan eksekusi kode saat memuat (berbeda dengan `pickle`).\n",
    "2.  **Performa**: Mendukung *zero-copy* dan sangat cepat dalam proses I/O.\n",
    "3.  **Interoperabilitas**: Kompatibel dengan berbagai framework (PyTorch, JAX, TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68efa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Memuat Model yang Sudah Disimpan\n",
    "# 1. Inisialisasi model baru dengan struktur yang sama\n",
    "model_baru = ConvNet(1, num_classes, rngs=nnx.Rngs(SEED))\n",
    "\n",
    "# 2. Muat parameter dari checkpoint epoch terakhir (misalnya epoch 5)\n",
    "load_checkpoint(model_baru, f\"{checkpoint_dir}/epoch_5.safetensors\")\n",
    "\n",
    "# 3. Verifikasi dengan melakukan inferensi pada satu batch\n",
    "images, labels = next(iter(test_loader))\n",
    "logits_orig = model(images)\n",
    "logits_load = model_baru(images)\n",
    "\n",
    "# Cek apakah hasilnya sama\n",
    "diff = jnp.abs(logits_orig - logits_load).max()\n",
    "print(f\"Selisih maksimum output: {diff}\")\n",
    "if diff < 1e-5:\n",
    "    print(\"Verifikasi Berhasil: Model yang dimuat memiliki output yang identik!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
