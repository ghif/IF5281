{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generative Adversarial Networks (GANs) menggunakan JAX dan Flax\n",
                "\n",
                "Generative Adversarial Networks (GANs) adalah kelas kerangka kerja pembelajaran mesin yang dirancang oleh Ian Goodfellow dan rekan-rekannya pada tahun 2014. GAN terdiri dari dua jaringan saraf, yaitu **Generator** dan **Discriminator**, yang bersaing satu sama lain dalam permainan zero-sum.\n",
                "\n",
                "## 1. Latar Belakang Matematis\n",
                "\n",
                "Proses pelatihan GAN sering digambarkan sebagai permainan minimax di mana Generator ($G$) mencoba menghasilkan sampel realistis untuk menipu Discriminator ($D$), dan Discriminator mencoba membedakan antara data asli dan sampel palsu yang dihasilkan oleh Generator.\n",
                "\n",
                "Fungsi objektif utamanya adalah **Value Function** $V(D, G)$:\n",
                "\n",
                "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
                "\n",
                "Di mana:\n",
                "- $x$: sebuah gambar asli dari data pelatihan.\n",
                "- $z$: sebuah vektor noise acak dari distribusi prior (misalnya, Gaussian).\n",
                "- $G(z)$: representasi data dari generator.\n",
                "- $D(x)$: estimasi discriminator tentang probabilitas bahwa contoh data asli $x$ adalah asli.\n",
                "\n",
                "### Langkah Pelatihan\n",
                "1. **Perbarui Discriminator**: Maksimalkan probabilitas penempatan label yang benar baik untuk contoh pelatihan maupun sampel dari $G$.\n",
                "2. **Perbarui Generator**: Minimalkan $\\log(1 - D(G(z)))$. Dalam praktiknya, kita sering kali memaksimalkan $\\log D(G(z))$ untuk memberikan gradien yang lebih baik di awal pelatihan."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Persiapan dan Impor\n",
                "\n",
                "Pertama, mari kita impor pustaka yang diperlukan. Kita akan menggunakan JAX untuk komputasi backend, Flax NNX untuk definisi model, dan Optax untuk optimasi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from flax import nnx\n",
                "import matplotlib.pyplot as plt\n",
                "import sys, os\n",
                "import numpy as np\n",
                "import time as timer\n",
                "from tqdm import tqdm\n",
                "import grain.python as grain\n",
                "from sklearn.datasets import fetch_openml\n",
                "import optax\n",
                "\n",
                "# Add parent directory to path to import utils\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
                "import viz_utils as vu\n",
                "import model_utils as mu"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Konfigurasi dan Hiperparameter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define constants\n",
                "DATA_DIR = \"/Users/mghifary/Work/Code/AI/data\"\n",
                "MODEL_DIR = \"/Users/mghifary/Work/Code/AI/IF5281/jax/models\" \n",
                "\n",
                "BATCH_SIZE = 128\n",
                "NUM_EPOCH = 50\n",
                "NC = 1 # num channels\n",
                "NZ = 100 # num latent variables\n",
                "LR = 2e-4 # learning rate\n",
                "BETA1 = 0.5 # beta1 for Adam optimizer\n",
                "NVIZ = 64\n",
                "\n",
                "DATASET = 'mnist'\n",
                "mname = \"gan\"\n",
                "checkpoint_dir = os.path.join(MODEL_DIR, f\"{mname}_{DATASET}_z{NZ}\")\n",
                "sample_dir = os.path.join(checkpoint_dir, \"samples\")\n",
                "\n",
                "for d in [checkpoint_dir, sample_dir]:\n",
                "    if not os.path.exists(d):\n",
                "        os.makedirs(d)\n",
                "        print(f'The new directory {d} has been created')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Pemuatan Data\n",
                "\n",
                "Kita memuat dataset MNIST menggunakan `fetch_openml` dari Scikit-Learn dan menggunakan `grain` dari Google untuk pemuatan data yang efisien."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading MNIST via OpenML (may take a minute)...Found locally if cache exists\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading MNIST via OpenML (may take a minute)...Found locally if cache exists\")\n",
                "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='liac-arff')\n",
                "X_all, y_all = mnist.data, mnist.target.astype(np.int32)\n",
                "\n",
                "# Split into train and test (60k / 10k)\n",
                "X_train_all, X_test_all = X_all[:60000], X_all[60000:]\n",
                "y_train_all, y_test_all = y_all[:60000], y_all[60000:]\n",
                "\n",
                "class MNISTSource(grain.RandomAccessDataSource):\n",
                "    def __init__(self, images, labels):\n",
                "        self._images = images\n",
                "        self._labels = labels\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self._images)\n",
                "        \n",
                "    def __getitem__(self, index):\n",
                "        # Reshape to (1, 28, 28) and normalize to [-1, 1]\n",
                "        image = (self._images[index].reshape(1, 28, 28).astype(np.float32) / 255.0) * 2.0 - 1.0\n",
                "        label = self._labels[index]\n",
                "        return {'image': image, 'label': label}\n",
                "\n",
                "def create_loader(data_source, batch_size, shuffle=False, seed=0):\n",
                "    sampler = grain.IndexSampler(\n",
                "        num_records=len(data_source),\n",
                "        shard_options=grain.NoSharding(),\n",
                "        shuffle=shuffle,\n",
                "        num_epochs=1,\n",
                "        seed=seed,\n",
                "    )\n",
                "    dataloader = grain.DataLoader(\n",
                "        data_source=data_source,\n",
                "        sampler=sampler,\n",
                "        worker_count=0,\n",
                "    )\n",
                "    \n",
                "    class BatchIterator:\n",
                "        def __init__(self, loader, batch_size, num_records):\n",
                "            self.loader = loader\n",
                "            self.batch_size = batch_size\n",
                "            self.num_records = num_records\n",
                "        \n",
                "        def __len__(self):\n",
                "            return (self.num_records + self.batch_size - 1) // self.batch_size\n",
                "\n",
                "        def __iter__(self):\n",
                "            batch_images, batch_labels = [], []\n",
                "            for record in self.loader:\n",
                "                batch_images.append(record['image'])\n",
                "                batch_labels.append(record['label'])\n",
                "                if len(batch_images) == self.batch_size:\n",
                "                    yield np.stack(batch_images), np.array(batch_labels)\n",
                "                    batch_images, batch_labels = [], []\n",
                "            if batch_images:\n",
                "                 yield np.stack(batch_images), np.array(batch_labels)\n",
                "    \n",
                "    return BatchIterator(dataloader, batch_size, len(data_source))\n",
                "\n",
                "train_loader = create_loader(MNISTSource(X_train_all, y_train_all), BATCH_SIZE, shuffle=True, seed=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Arsitektur Model\n",
                "\n",
                "Baik Generator maupun Discriminator adalah Multi-Layer Perceptrons (MLPs) sebagaimana didefinisikan dalam makalah asli GAN untuk dataset sederhana seperti MNIST."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Generator(nnx.Module):\n",
                "    def __init__(self, input_size=100, output_size=784, rngs: nnx.Rngs = None):\n",
                "        self.fc1 = nnx.Linear(input_size, 128, rngs=rngs)\n",
                "        self.fc2 = nnx.Linear(128, 256, rngs=rngs)\n",
                "        self.bn2 = nnx.BatchNorm(256, rngs=rngs)\n",
                "        self.fc3 = nnx.Linear(256, 512, rngs=rngs)\n",
                "        self.bn3 = nnx.BatchNorm(512, rngs=rngs)\n",
                "        self.fc4 = nnx.Linear(512, 1024, rngs=rngs)\n",
                "        self.bn4 = nnx.BatchNorm(1024, rngs=rngs)\n",
                "        self.fc5 = nnx.Linear(1024, output_size, rngs=rngs)\n",
                "    \n",
                "    def __call__(self, z):\n",
                "        h = nnx.leaky_relu(self.fc1(z), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn2(self.fc2(h)), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn3(self.fc3(h)), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.bn4(self.fc4(h)), negative_slope=0.2)\n",
                "        h = nnx.tanh(self.fc5(h))\n",
                "        return h.reshape(h.shape[0], NC, 28, 28)\n",
                "\n",
                "class Discriminator(nnx.Module):\n",
                "    def __init__(self, input_size=784, num_classes=1, rngs: nnx.Rngs = None):\n",
                "        self.fc1 = nnx.Linear(input_size, 512, rngs=rngs)\n",
                "        self.fc2 = nnx.Linear(512, 256, rngs=rngs)\n",
                "        self.fc3 = nnx.Linear(256, num_classes, rngs=rngs)\n",
                "        \n",
                "    def __call__(self, x):\n",
                "        x = x.reshape(x.shape[0], -1) \n",
                "        h = nnx.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
                "        h = nnx.leaky_relu(self.fc2(h), negative_slope=0.2)\n",
                "        return nnx.sigmoid(self.fc3(h)).flatten()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Fungsi Pelatihan\n",
                "\n",
                "Kita mendefinisikan fungsi loss dan langkah pelatihan yang dikompilasi JIT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def binary_cross_entropy(logits, labels):\n",
                "    epsilon = 1e-12\n",
                "    logits = jnp.clip(logits, epsilon, 1.0 - epsilon)\n",
                "    return -jnp.mean(labels * jnp.log(logits) + (1 - labels) * jnp.log(1 - logits))\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_D(netD, netG, optimizerD, real_x, noise):\n",
                "    fake_x = netG(noise)\n",
                "    def loss_fn(netD):\n",
                "        real_pred = netD(real_x)\n",
                "        fake_pred = netD(fake_x)\n",
                "        errD = binary_cross_entropy(real_pred, jnp.ones_like(real_pred)) + \\\n",
                "               binary_cross_entropy(fake_pred, jnp.zeros_like(fake_pred))\n",
                "        return errD, (real_pred, fake_pred)\n",
                "    (loss, (real_p, fake_p)), grads = nnx.value_and_grad(loss_fn, has_aux=True)(netD)\n",
                "    optimizerD.update(netD, grads)\n",
                "    return loss, jnp.mean(real_p), jnp.mean(fake_p)\n",
                "\n",
                "@nnx.jit\n",
                "def train_step_G(netD, netG, optimizerG, noise):\n",
                "    def loss_fn(netG):\n",
                "        fake_x = netG(noise)\n",
                "        preds = netD(fake_x)\n",
                "        return binary_cross_entropy(preds, jnp.ones_like(preds)), preds\n",
                "    (loss, preds), grads = nnx.value_and_grad(loss_fn, has_aux=True)(netG)\n",
                "    optimizerG.update(netG, grads)\n",
                "    return loss, jnp.mean(preds)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Loop Pelatihan\n",
                "\n",
                "Sekarang kita menginisialisasi model dan menjalankan loop pelatihan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rngs = nnx.Rngs(0)\n",
                "netG = Generator(input_size=NZ, output_size=28*28, rngs=rngs)\n",
                "netD = Discriminator(input_size=28*28, num_classes=1, rngs=rngs)\n",
                "\n",
                "optimizerG = nnx.Optimizer(netG, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "optimizerD = nnx.Optimizer(netD, optax.adam(LR, b1=BETA1), wrt=nnx.Param)\n",
                "\n",
                "fixed_latent = jax.random.normal(jax.random.PRNGKey(42), (64, NZ))\n",
                "step_rng = jax.random.PRNGKey(0)\n",
                "\n",
                "for epoch in range(NUM_EPOCH):\n",
                "    start_t = timer.time()\n",
                "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}\") as tepoch:\n",
                "        for batch_idx, (real_x, _) in enumerate(tepoch):\n",
                "            step_rng, rng_d, rng_g = jax.random.split(step_rng, 3)\n",
                "            \n",
                "            noise_d = jax.random.normal(rng_d, (real_x.shape[0], NZ))\n",
                "            errD, D_x, D_G_z1 = train_step_D(netD, netG, optimizerD, real_x, noise_d)\n",
                "            \n",
                "            noise_g = jax.random.normal(rng_g, (real_x.shape[0], NZ))\n",
                "            errG, D_G_z2 = train_step_G(netD, netG, optimizerG, noise_g)\n",
                "            \n",
                "            if batch_idx % 100 == 0:\n",
                "                tepoch.set_postfix(Loss_D=f\"{errD:.4f}\", Loss_G=f\"{errG:.4f}\", Dx=f\"{D_x:.4f}\")\n",
                "    \n",
                "    print(f'Epoch {epoch+1} finished in {timer.time() - start_t:.2f}s')\n",
                "\n",
                "    # Save Real Samples (Only once)\n",
                "    if epoch == 0:\n",
                "        vutils_real = real_x[:NVIZ] if real_x.shape[0] >= NVIZ else real_x\n",
                "        # real_x is (B, 1, 28, 28)\n",
                "        grid_real = vu.set_grid(vutils_real, num_cells=64) # NVIZ used in original was 512, let's use 64\n",
                "        plt.figure(figsize=(10, 10))\n",
                "        plt.imshow(np.transpose(np.array(vu.normalize(grid_real, 0, 1)), (1, 2, 0)), cmap='gray')\n",
                "        plt.axis('off')\n",
                "        plt.show()\n",
                "        plt.savefig(os.path.join(sample_dir, 'real_samples.jpg'), bbox_inches='tight')\n",
                "        plt.close()\n",
                "\n",
                "    if epoch % 10 != 0:\n",
                "        continue\n",
                "\n",
                "    # Save Fake Samples\n",
                "    fake_samples = netG(fixed_latent)\n",
                "    # fake_samples is (B, 1, 28, 28)\n",
                "    grid_fake = vu.set_grid(fake_samples, num_cells=64)\n",
                "    plt.figure(figsize=(10, 10))\n",
                "    plt.imshow(np.transpose(np.array(vu.normalize(grid_fake, 0, 1)), (1, 2, 0)), cmap='gray')\n",
                "    plt.axis('off')\n",
                "    plt.savefig(os.path.join(sample_dir, f'fake_samples_epoch-{epoch+1}.jpg'), bbox_inches='tight')\n",
                "    plt.show()\n",
                "    plt.close()\n",
                "    \n",
                "    # Checkpointing (Save mostly locally or periodically)\n",
                "    mu.save_checkpoint(netD, epoch + 1, filedir=checkpoint_dir) # Might need to differentiate dicts if saving both in one dir\n",
                "    # wait, model_utils save_checkpoint uses 'epoch_{epoch}.safetensors'. \n",
                "    # If we run this for both, they overwrite.\n",
                "    # We should probably modify save_checkpoint to accept prefix or handle separate folders.\n",
                "    # For now, let's save G and D in separate subfolders or just rename manually/hackily?\n",
                "    # Or just use the model_utils standard and assume single model.\n",
                "    # Let's save them as:\n",
                "    \n",
                "    # Saving Generator\n",
                "    path_g = os.path.join(checkpoint_dir, \"generator\")\n",
                "    if not os.path.exists(path_g): os.makedirs(path_g)\n",
                "    mu.save_checkpoint(netG, epoch + 1, filedir=path_g)\n",
                "    \n",
                "    # Saving Discriminator\n",
                "    path_d = os.path.join(checkpoint_dir, \"discriminator\")\n",
                "    if not os.path.exists(path_d): os.makedirs(path_d)\n",
                "    mu.save_checkpoint(netD, epoch + 1, filedir=path_d)\n",
                "    \n",
                "    print(f\" --- Models stored ---\")\n",
                "\n",
                "\n",
                "print(\" -- Storing final checkpoints --\")\n",
                "mu.save_checkpoint(netD, epoch + 1, filedir=checkpoint_dir)\n",
                "mu.save_checkpoint(netG, epoch + 1, filedir=path_g)\n",
                "mu.save_checkpoint(netD, epoch + 1, filedir=path_d)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "jax-cpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
