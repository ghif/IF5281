{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c218c421",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0195d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b1188e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_Softargmax = nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec01f2",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5238c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, p, d_input=None):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention Layer\n",
    "        \n",
    "        d_model (int):  \n",
    "        num_heads (int):\n",
    "        p (int):\n",
    "        d_input (int): \n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of the number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.d_k = d_model // self.num_heads\n",
    "        \n",
    "        # These are still of dimension d_model. They will be split into number of heads\n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        k_length = K.size(-2)\n",
    "        \n",
    "        # Scaling by d_k so that the soft(arg)max doesn't saturate\n",
    "        Q = Q / np.sqrt(self.d_k) # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(Q, K.transpose(2,3)) # (bs, n_heads, q_length, k_lengt)\n",
    "        \n",
    "        A = nn_Softargmax(dim=-1)(scores) # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        # Get the weighted average of the values\n",
    "        H = torch.matmul(A, V) # (bs, n_heads, q_length, dim_per_head)\n",
    "        \n",
    "        return H, A\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (heads X depth)\n",
    "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            H (bs, q_length, dim)\n",
    "            A ()\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, dim = X_q.size()\n",
    "        \n",
    "        # After transforming, split into num_heads\n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size) # (bs, n_heads, q_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size) # (bs, n_heads, k_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size) # (bs, n_heads, v_length, dim_per_head)\n",
    "        \n",
    "        # Calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat, batch_size) # (bs, q_length, dim)\n",
    "        \n",
    "        # Final linear layer\n",
    "        H = self.W_h(H_cat) # (bs, q_length, dim)\n",
    "        \n",
    "        return H, A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f10ba",
   "metadata": {},
   "source": [
    "### Just a sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9099d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8, p=0)\n",
    "def print_out(Q, K, V):\n",
    "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
    "    print('Attention weights are: ', temp_attn.squeeze())\n",
    "    print('Output is:', temp_out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af09c",
   "metadata": {},
   "source": [
    "To check our self attention works - if the query matches with one of the key values, it should have all the attention focused there, with the value returned as the value at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1db53467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:  tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output is: tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0],\n",
    "    [0, 10, 0],\n",
    "    [0, 0, 10],\n",
    "    [0, 0, 10]]\n",
    ").float()[None, None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[1, 0, 0],\n",
    "     [10, 0, 0],\n",
    "     [100, 5, 0],\n",
    "     [1000, 6, 0]\n",
    "    ]\n",
    ").float()[None, None]\n",
    "\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]]\n",
    ").float()[None, None]\n",
    "\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067c695",
   "metadata": {},
   "source": [
    "We can see that if focuses on the second key and returns the second value.\n",
    "\n",
    "If we give a query that matches two keys exactly, it should return the averaged value of the two values of those two keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c97c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:  tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output is: tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor([[0, 0, 10]]).float()\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae0ce9",
   "metadata": {},
   "source": [
    "We see that it focuses equally on the 3rd and 4th key and reutnrs the average of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdafa09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:  tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
      "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
      "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
      "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
      "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
      "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Now giving all the queries at the same time\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10],\n",
    "     [0, 10, 0],\n",
    "     [10, 10, 0],\n",
    "    ]\n",
    ").float()[None, None]\n",
    "\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6925f30",
   "metadata": {},
   "source": [
    "### 1D convolution with kernel_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae3e2d",
   "metadata": {},
   "source": [
    "This is basically an MLP with 1 hidden layer and ReLU activation applied to each and every element in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27b7000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, p):\n",
    "        super().__init__()\n",
    "        self.klconvL1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.klconvL2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.klconvL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.klconvL2(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c6f89",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19251974",
   "metadata": {},
   "source": [
    "Now we have all components for our Transformer Encoder block shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e36b3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim, p=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, p)\n",
    "        self.cnn = CNN1d(d_model, conv_hidden_dim, p)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.mha(x, x, x) # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Layer norm after adding the residual connection\n",
    "        out1 = self.layernorm1(x + attn_output) # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Feed forward\n",
    "        cnn_output = self.cnn(out1) # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Second layer norm after adding residual connection\n",
    "        out2 = self.layernorm2(out1 + cnn_output) # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c8049",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "#### Blocks of N Encoder Layers + Positional encoding + Inut embedding\n",
    "\n",
    "Self attention by itself does not have any recurrence or convlutions so to make it sensitive to position we must provide additional positional encodings. These are calculated as follows:\n",
    "\n",
    "$$E(p, 2i) = \\sin(p/10000^{2i/d})$$\n",
    "$$E(p, 2i+1) = \\cos(p/10000^(2i/d)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40043592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
    "    E.requires_grad = False\n",
    "    \n",
    "    theta = np.array([\n",
    "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for p in range(nb_p)\n",
    "    ])\n",
    "    \n",
    "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
    "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
    "    \n",
    "    E = E.to(device)\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        create_sinusoidal_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        print(f'[Embeddings] input_idx: {input_ids.size()}')\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # Get position embeddings for each position id\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Add them both\n",
    "        embeddings = word_embeddings + position_embeddings\n",
    "        \n",
    "        # Layer norm\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a777752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size, maximum_position_encoding, p=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = Embeddings(d_model, input_vocab_size, maximum_position_encoding, p)\n",
    "        \n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim, p))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "            \n",
    "        return x # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b1de350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f16abffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_data = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf7903d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd285cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1ca94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8faaa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9cad08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# class: 4\n",
      "vocab size: 95811\n",
      "conv_hidden_dim: 64\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_data]))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "conv_hidden_dim = 64\n",
    "print(f'# class: {num_class}')\n",
    "print(f'vocab size: {vocab_size}')\n",
    "print(f'conv_hidden_dim: {conv_hidden_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8cd1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, \n",
    "            d_model, \n",
    "            num_heads, \n",
    "            conv_hidden_dim, \n",
    "            input_vocab_size, \n",
    "            maximum_position_encoding=10000\n",
    "        )\n",
    "        self.dense = nn.Linear(d_model, num_answers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "713d3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embeddings): Embedding(95811, 32, padding_idx=1)\n",
       "      (position_embeddings): Embedding(10000, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (cnn): CNN1d(\n",
       "          (klconvL1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (klconvL2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2,\n",
    "                              conv_hidden_dim=64, input_vocab_size=vocab_size, num_answers=2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd83ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        print(f'text shape: {text.size()}')\n",
    "        print(f'text : {text}')\n",
    "        print(f'label shape: {label.size()}')\n",
    "        print(f'label: {label}')\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39f52124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "997d06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dde6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "969f1b38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'legacy' from 'torchtext' (/Users/mghifary/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchtext/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 36\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m legacy\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'legacy' from 'torchtext' (/Users/mghifary/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchtext/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torchtext import legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75b36969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text shape: torch.Size([2767])\n",
      "text : tensor([ 8775, 31823,  2578,  ...,     7,   950,     1])\n",
      "label shape: torch.Size([64])\n",
      "label: tensor([0, 3, 1, 2, 1, 1, 2, 1, 2, 3, 0, 0, 1, 1, 2, 3, 2, 1, 0, 2, 1, 3, 3, 0,\n",
      "        2, 0, 3, 0, 1, 3, 1, 2, 1, 3, 0, 2, 3, 3, 1, 1, 2, 3, 2, 3, 2, 0, 3, 0,\n",
      "        1, 2, 1, 0, 0, 1, 3, 2, 3, 1, 2, 2, 2, 1, 1, 1])\n",
      "[Embeddings] input_idx: torch.Size([2767])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 36\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train(train_dataloader, criterion, optimizer, epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     accu_val \u001b[39m=\u001b[39m evaluate(valid_dataloader, criterion)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m total_accu \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m total_accu \u001b[39m>\u001b[39m accu_val:\n",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 36\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel shape: \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel: \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m model(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predicted_label, label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 36\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(x, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 36\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x) \u001b[39m# Transform to (batch_size, input_seq_length, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_layers[i](x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 36\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[Embeddings] input_idx: \u001b[39m\u001b[39m{\u001b[39;00minput_ids\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     seq_length \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     position_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(seq_length, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39minput_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand_as(input_ids)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, criterion, optimizer, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, criterion)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    data_iterator = iter(data_loader)\n",
    "    nb_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    acc = 0 \n",
    "    for batch in data_iterator:\n",
    "        x = batch.text.to(device)\n",
    "        y = batch.label.to(device)\n",
    "                \n",
    "        out = model(x)\n",
    "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "\n",
    "    print(f\"Eval accuracy: {acc / nb_batches}\")\n",
    "    \n",
    "def train(train_loader, valid_loader):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
    "        nb_batches_train = len(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            x = batch.text.to(device)\n",
    "            y = batch.label.to(device)\n",
    "            \n",
    "            out = model(x)  # ①\n",
    "\n",
    "            loss = f.cross_entropy(out, y)  # ②\n",
    "            \n",
    "            model.zero_grad()  # ③\n",
    "\n",
    "            loss.backward()  # ④\n",
    "            losses += loss.item()\n",
    "\n",
    "            optimizer.step()  # ⑤\n",
    "                        \n",
    "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "        \n",
    "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
    "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
    "        print('Evaluating on validation:')\n",
    "        evaluate(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ec2c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb Cell 32\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m max_len \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mField(sequential\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fix_length\u001b[39m=\u001b[39mmax_len, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m label \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mLabelField(sequential\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mghifary/Work/govtech/codes/AI/IF5281/7-transformers.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ds_train, ds_test \u001b[39m=\u001b[39m legacy\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mIMDB\u001b[39m.\u001b[39msplits(text, label, root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "max_len = 200\n",
    "text = data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
    "label = data.LabelField(sequential=False, dtype=torch.long)\n",
    "ds_train, ds_test = legacy.datasets.IMDB.splits(text, label, root='./')\n",
    "print('train : ', len(ds_train))\n",
    "print('test : ', len(ds_test))\n",
    "print('train.fields :', ds_train.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b537366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
