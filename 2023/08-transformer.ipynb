{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76439,
     "status": "ok",
     "timestamp": 1680764223941,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "AH2o6LMFCrHj",
    "outputId": "fea85fed-c108-4685-e7e0-bf0d5e3cf733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.0 torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w6O-s3wGvPB"
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDfWtRfjG7Fw"
   },
   "source": [
    "To make the following code works, make sure that torch<=1.8.0 and torchtext<=0.9.0 are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1680764224665,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "1-hgSH2rDtsj"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1680764224666,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "FGo2zagFGhNg"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_Softargmax = nn.Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2jJ8rEtGoFX"
   },
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1680764224666,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "-WEV2kqFGsTX"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, p, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // self.num_heads\n",
    "        \n",
    "        # These are still of dimension d_model. They will be split into number of heads \n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        batch_size = Q.size(0) \n",
    "        k_length = K.size(-2) \n",
    "        \n",
    "        # Scaling by d_k so that the soft(arg)max doesnt saturate\n",
    "        Q = Q / np.sqrt(self.d_k)                         # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(Q, K.transpose(2,3))          # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        A = nn_Softargmax(dim=-1)(scores)   # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        # Get the weighted average of the values\n",
    "        H = torch.matmul(A, V)     # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "        return H, A \n",
    "\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (heads X depth)\n",
    "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size, seq_length, dim = X_q.size()\n",
    "\n",
    "        # After transforming, split into num_heads \n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head)\n",
    "        \n",
    "        # Calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat, batch_size)    # (bs, q_length, dim)\n",
    "        \n",
    "        # Final linear layer  \n",
    "        H = self.W_h(H_cat)          # (bs, q_length, dim)\n",
    "        \n",
    "        return H, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DcYwz_3HVin"
   },
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1680764224666,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "53Udg6CuHWCN"
   },
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8, p=0)\n",
    "def print_out(Q, K, V):\n",
    "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
    "    print('Attention weights are:', temp_attn.squeeze())\n",
    "    print('Output is:', temp_out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1680764224666,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "pwwOjI8UHY4X",
    "outputId": "9fe56026-9849-4a31-c134-7db9855e2eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output is: tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0],\n",
    "     [ 0,10, 0],\n",
    "     [ 0, 0,10],\n",
    "     [ 0, 0,10]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0],\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]]\n",
    ").float()[None,None]\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX7zBgzAHqCu"
   },
   "source": [
    "We can see that it focuses on the second key and returns the second value.\n",
    "\n",
    "If we give a query that matches two keys exactly, it should return the averaged value of the two values for those two keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1680764224667,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "07ej3sJaHzSv",
    "outputId": "ee2dbe31-4d2c-4c79-9653-756ddc9eebec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output is: tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor([[0, 0, 10]]).float()  \n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arc1EMDnIKW_"
   },
   "source": [
    "Now giving all the queries at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1680764224667,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "naE810P3IRcp",
    "outputId": "9d03dab3-ebb7-4a06-9266-d9dfa7aa0fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
      "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
      "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
      "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
      "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
      "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
    ").float()[None,None]\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwIYUb_PHUWO"
   },
   "source": [
    "## 1D convolution with kernel_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o6qeOPpIaEJ"
   },
   "source": [
    "This is basically an MLP with one hidden layer and ReLU activation applied to each and every element in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1680764224667,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "jqCd0fhsIaic"
   },
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, p):\n",
    "        super().__init__()\n",
    "        self.k1convL1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.k1convL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.k1convL2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSnfehGCIgK5"
   },
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPa3Vn2tInB4"
   },
   "source": [
    "Now we have all components for our Transformer Encoder block shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1680764224667,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "cFQW798HJJhj"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, p)\n",
    "        self.cnn = CNN1d(d_model, conv_hidden_dim, p)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Multi-head attention \n",
    "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Layer norm after adding the residual connection \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Feed forward \n",
    "        cnn_output = self.cnn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        #Second layer norm after adding residual connection \n",
    "        out2 = self.layernorm2(out1 + cnn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XKe886vJP0N"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "**Blocks of N encoder layers + positional encoding + input embedding.**\n",
    "\n",
    "Self attention by itself does not have any recurrence or convolutions so to make it sensitive to position we must provide additional positional encodings. These are calculated as follows:\n",
    "\n",
    "$$E(p, 2i) = \\sin (p/10000^{2i/d})$$\n",
    "$$E(p, 2i+1) = \\cos (p/10000^{2i/d})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1680764224668,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "nYslGzKwJkrZ"
   },
   "outputs": [],
   "source": [
    "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
    "    E.requires_grad = False\n",
    "    theta = np.array([\n",
    "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for p in range(nb_p)\n",
    "    ])\n",
    "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
    "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
    "    \n",
    "    E = E.to(device)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        create_sinusoidal_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (bs, max_seq_length)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Get position embeddings for each position id \n",
    "        position_embeddings = self.position_embeddings(position_ids)        # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Add them both \n",
    "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Layer norm \n",
    "        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1680764224669,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "L1ohKLsMJo1h"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size,\n",
    "               maximum_position_encoding, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding, p)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim, p))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1NgolCAJ1jk"
   },
   "source": [
    "## Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1680764224669,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "8v_Hwm9iDTl0"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'legacy' from 'torchtext' (/Users/mghifary/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchtext/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m legacy\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'legacy' from 'torchtext' (/Users/mghifary/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchtext/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torchtext import legacy\n",
    "from torchtext.legacy import data\n",
    "import torchtext.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44326,
     "status": "ok",
     "timestamp": 1680764268986,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "B23xjYhcEH1U",
    "outputId": "cd475418-a0b2-46cb-dbb8-18d4f7fb33cf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'legacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m----> 2\u001b[0m text \u001b[38;5;241m=\u001b[39m legacy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fix_length\u001b[38;5;241m=\u001b[39mmax_len, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m label \u001b[38;5;241m=\u001b[39m legacy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mLabelField(sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      4\u001b[0m ds_train, ds_test \u001b[38;5;241m=\u001b[39m legacy\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mIMDB\u001b[38;5;241m.\u001b[39msplits(text, label, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'legacy' is not defined"
     ]
    }
   ],
   "source": [
    "max_len = 200\n",
    "text = legacy.data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
    "label = legacy.data.LabelField(sequential=False, dtype=torch.long)\n",
    "ds_train, ds_test = legacy.datasets.IMDB.splits(text, label, root='./')\n",
    "print('train : ', len(ds_train))\n",
    "print('test : ', len(ds_test))\n",
    "print('train.fields :', ds_train.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1680764268987,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "7TRRY6nIEIik",
    "outputId": "f1532f0e-8806-4154-c938-5f9f7bc127c4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_train, ds_valid \u001b[38;5;241m=\u001b[39m ds_train\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ds_train))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ds_valid))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "ds_train, ds_valid = ds_train.split(0.9)\n",
    "print('train: ', len(ds_train))\n",
    "print('valid: ', len(ds_valid))\n",
    "print('test: ', len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2368,
     "status": "ok",
     "timestamp": 1680764271334,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "G55A6eIPExkj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50_000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m text\u001b[38;5;241m.\u001b[39mbuild_vocab(ds_train, max_size\u001b[38;5;241m=\u001b[39mnum_words)\n\u001b[1;32m      3\u001b[0m label\u001b[38;5;241m.\u001b[39mbuild_vocab(ds_test)\n\u001b[1;32m      4\u001b[0m vocab \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mvocab\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "num_words = 50_000\n",
    "text.build_vocab(ds_train, max_size=num_words)\n",
    "label.build_vocab(ds_test)\n",
    "vocab = text.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1680764271334,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "I9N9-dwjFHGr"
   },
   "outputs": [],
   "source": [
    "batch_size = 164\n",
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680764271334,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "uc0CrCMaJ96j"
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size,\n",
    "                         maximum_position_encoding=10000)\n",
    "        self.dense = nn.Linear(d_model, num_answers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1564,
     "status": "ok",
     "timestamp": 1680764272895,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "kRSllOAyKARa",
    "outputId": "0f1526f2-e658-40e6-cff5-87f74cd542f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
       "      (position_embeddings): Embedding(10000, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (cnn): CNN1d(\n",
       "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2, \n",
    "                         conv_hidden_dim=128, input_vocab_size=50002, num_answers=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1680764272895,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "1cN2mZ4DKFqQ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "t_total = len(train_loader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1680764272896,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "hwntDXjIKWl5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate(data_loader):\n",
    "    data_iterator = iter(data_loader)\n",
    "    nb_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    acc = 0 \n",
    "    for batch in data_iterator:\n",
    "        x = batch.text.to(device)\n",
    "        y = batch.label.to(device)\n",
    "                \n",
    "        out = model(x)\n",
    "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "\n",
    "    print(f\"Eval accuracy: {acc / nb_batches}\")\n",
    "\n",
    "def train(train_loader, valid_loader):\n",
    "    for epoch in range(epochs):\n",
    "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
    "        nb_batches_train = len(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "\n",
    "        start_t = time.time()\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            x = batch.text.to(device)\n",
    "            y = batch.label.to(device)\n",
    "            \n",
    "            out = model(x)  # ①\n",
    "\n",
    "            loss = f.cross_entropy(out, y)  # ②\n",
    "            \n",
    "            model.zero_grad()  # ③\n",
    "\n",
    "            loss.backward()  # ④\n",
    "            losses += loss.item()\n",
    "\n",
    "            optimizer.step()  # ⑤\n",
    "                        \n",
    "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "        \n",
    "        elapsed_t = time.time() - start_t\n",
    "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
    "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
    "        print(f\"Training time: {elapsed_t:.3f}\")\n",
    "        print('Evaluating on validation:')\n",
    "        evaluate(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 619463,
     "status": "ok",
     "timestamp": 1680764892355,
     "user": {
      "displayName": "Muhammad Ghifary",
      "userId": "13617895030075015337"
     },
     "user_tz": -420
    },
    "id": "LISWLb0TKdLz",
    "outputId": "efbc57f5-b993-49cc-b715-8ed2360b9779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0 is 0.7334402937819993\n",
      "Training accuracy: 0.5381539413220221\n",
      "Training time: 66.351\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.5711509146341465\n",
      "Training loss at epoch 1 is 0.6432446634423905\n",
      "Training accuracy: 0.6429447242841989\n",
      "Training time: 60.432\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.6562118902439024\n",
      "Training loss at epoch 2 is 0.5839844972327135\n",
      "Training accuracy: 0.701931998939554\n",
      "Training time: 55.071\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7050304878048782\n",
      "Training loss at epoch 3 is 0.49138594904671545\n",
      "Training accuracy: 0.7685301785083071\n",
      "Training time: 60.889\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7561737804878049\n",
      "Training loss at epoch 4 is 0.40221595267454785\n",
      "Training accuracy: 0.8189510427712973\n",
      "Training time: 53.843\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7735899390243901\n",
      "Training loss at epoch 5 is 0.33827512149793515\n",
      "Training accuracy: 0.8547355514316011\n",
      "Training time: 57.709\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.798094512195122\n",
      "Training loss at epoch 6 is 0.2839992653200592\n",
      "Training accuracy: 0.8843617444326617\n",
      "Training time: 59.421\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.803125\n",
      "Training loss at epoch 7 is 0.23805676314277927\n",
      "Training accuracy: 0.9067526069282439\n",
      "Training time: 56.878\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8197789634146343\n",
      "Training loss at epoch 8 is 0.19696390704400296\n",
      "Training accuracy: 0.9283205196182391\n",
      "Training time: 60.625\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8243902439024391\n",
      "Training loss at epoch 9 is 0.16107112288043118\n",
      "Training accuracy: 0.9437025008837044\n",
      "Training time: 60.935\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8277820121951219\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2HuBmRcM1u4WnFEyGLOK0",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
